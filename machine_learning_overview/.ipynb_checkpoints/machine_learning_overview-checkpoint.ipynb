{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI in general"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.tutorialspoint.com/artificial_intelligence/artificial_intelligence_overview.htm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A computer program without AI can answer the specific questions it is meant to solve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A computer program with AI can answer the generic questions it is meant to solve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Image(filename='ai_venn.jpg', width = 700) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Image(filename='function.png', width = 500) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy, Pandas, Scikit-Learn, Matplotlib, Tensorflow, PyTorch, Keras, OpenCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning (“ML“) is the scientific study of algorithms and statistical models that computer systems use to perform a specific task without using explicit instructions, relying instead on patterns and inference derived from data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Image(filename='Types-of-Machine-Learning-algorithms.jpg', width = 900)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An algorithm that learns to perform a task simply by trying to maximise rewards it receives for its actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This technique is most often used in game-like situations, e.g. playing games such as Go, self-driving cars, trading strategies, balancing electricity grid loads or optimising auction pricing in real time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Image(filename='Reinforcement_Learning.png', width = 700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/supervised_learning.html#supervised-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An algorithm uses training data and feedback from humans to learn the relationship of given inputs to desired outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data is labelled by humans, e.g. photo X = cat, photo Y = potato etc. The labeling, together with the human-influenced feedback loop to improve the machine-generated results, explains why we term this type of ML “supervised”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='ml_road_map.png', width = 900)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification is used to predict discrete responses, that is things with fixed values, e.g. number of students in a class (you can’t have half a student). In a legal context, ML classification algorithms are used to classify whether clause X describes French governing law or English governing law."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eventually, the performance of a classifier, computational power as well as\n",
    "predictive power, depends heavily on the underlying data that are available for\n",
    "learning. The five main steps that are involved in training a machine learning\n",
    "algorithm can be summarized as follows:\n",
    "1. Selection of features.\n",
    "2. Choosing a performance metric.\n",
    "3. Choosing a classifier and optimization algorithm.\n",
    "4. Evaluating the performance of the model.\n",
    "5. Tuning the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='perceptron_1.png', width = 900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='perceptron_2.png', width = 900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Image(filename='perceptron_3.png', width = 900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='cost_func.png', width = 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Image(filename='grad_desc.png', width = 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='grad_desc_math.png', width = 900)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@tiago.tmleite/neural-networks-multilayer-perceptron-and-the-backpropagation-algorithm-a5cd5b904fde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='backprop_math.png', width = 900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Perceptron(object):\n",
    "    \"\"\"Perceptron classifier.\n",
    "    Parameters\n",
    "    ------------\n",
    "    eta : float\n",
    "    Learning rate (between 0.0 and 1.0)\n",
    "    n_iter : int\n",
    "    Passes over the training dataset.\n",
    "    Attributes\n",
    "    -----------\n",
    "    w_ : 1d-array\n",
    "    Weights after fitting.\n",
    "    errors_ : list\n",
    "    Number of misclassifications in every epoch.\n",
    "    \"\"\"\n",
    "    def __init__(self, eta=0.01, n_iter=10):\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit training data.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like}, shape = [n_samples, n_features]\n",
    "        Training vectors, where n_samples\n",
    "        is the number of samples and\n",
    "        n_features is the number of features.\n",
    "        y : array-like, shape = [n_samples]\n",
    "        Target values.\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "        \"\"\"\n",
    "        self.w_ = np.zeros(1 + X.shape[1])\n",
    "        self.errors_ = []\n",
    "        for _ in range(self.n_iter):\n",
    "            errors = 0\n",
    "            for xi, target in zip(X, y):\n",
    "                update = self.eta * (target - self.predict(xi))\n",
    "                self.w_[1:] += update * xi\n",
    "                self.w_[0] += update\n",
    "                errors += int(update != 0.0)\n",
    "            self.errors_.append(errors)\n",
    "        return self\n",
    "    \n",
    "    def net_input(self, X):\n",
    "        \"\"\"Calculate net input\"\"\"\n",
    "        return np.dot(X, self.w_[1:]) + self.w_[0]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Return class label after unit step\"\"\"\n",
    "        return np.where(self.net_input(X) >= 0.0, 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', header=None)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first feature column (sepal length) and the third feature column (petal length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.rcParams[\"figure.figsize\"] = (15, 10)\n",
    "\n",
    "y = df.iloc[0:100, 4].values\n",
    "y = np.where(y == 'Iris-setosa', -1, 1)\n",
    "X = df.iloc[0:100, [0, 2]].values\n",
    "plt.scatter(X[:50, 0], X[:50, 1], color='red', marker='o', label='setosa')\n",
    "plt.scatter(X[50:100, 0], X[50:100, 1], color='blue', marker='x', label='versicolor')\n",
    "plt.xlabel('petal length')\n",
    "plt.ylabel('sepal length')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppn = Perceptron(eta=0.1, n_iter=10)\n",
    "ppn.fit(X, y)\n",
    "\n",
    "\"\"\"\n",
    "from sklearn.linear_model import Perceptron\n",
    "ppn = Perceptron(n_iter=40, eta0=0.1, random_state=0)\n",
    "ppn.fit(X, y)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, len(ppn.errors_) + 1), ppn.errors_, marker='o')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Number of misclassifications')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "def plot_decision_regions(X, y, classifier, test_idx=None, resolution=0.02):\n",
    "    # setup marker generator and color map\n",
    "    markers = ('s', 'x', 'o', '^', 'v')\n",
    "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "    # plot the decision surface\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution), np.arange(x2_min, x2_max, resolution))\n",
    "    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)\n",
    "    plt.xlim(xx1.min(), xx1.max())\n",
    "    plt.ylim(xx2.min(), xx2.max())\n",
    "    # plot class samples\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1], alpha=0.8, c=cmap(idx), marker=markers[idx], label=cl)\n",
    "        \n",
    "    # highlight test samples\n",
    "    if test_idx:\n",
    "        X_test, y_test = X[test_idx, :], y[test_idx]\n",
    "        plt.scatter(X_test[:, 0], X_test[:, 1], c='', alpha=1.0, linewidth=1, marker='o', s=55, label='test set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_regions(X, y, classifier=ppn)\n",
    "plt.xlabel('sepal length [cm]')\n",
    "plt.ylabel('petal length [cm]')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.adrianstoll.com/ml/perceptron-learning-algorithm/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ADAptive LInear NEuron (Adaline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='adaline.png', width = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM (support vector machine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another powerful and widely used learning algorithm is the support vector\n",
    "machine (SVM), which can be considered as an extension of the perceptron. Using\n",
    "the perceptron algorithm, we minimized misclassification errors. However, in SVMs,\n",
    "our optimization objective is to maximize the margin. The margin is defined as the\n",
    "distance between the separating hyperplane (decision boundary) and the training\n",
    "samples that are closest to this hyperplane, which are the so-called support vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='svm.png', width = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, [2, 3]]\n",
    "y = iris.target\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)\n",
    "\n",
    "X_combined_std = np.vstack((X_train_std, X_test_std))\n",
    "y_combined = np.hstack((y_train, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = SVC(kernel='linear', C=1.0, random_state=0)\n",
    "svm.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_decision_regions(X_combined_std, y_combined, classifier=svm, test_idx=range(105,150))\n",
    "plt.xlabel('petal length [standardized]')\n",
    "plt.ylabel('petal width [standardized]')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM with RBF kerner (Radial Basis Function kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XOR problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "X_xor = np.random.randn(200, 2)\n",
    "y_xor = np.logical_xor(X_xor[:, 0] > 0, X_xor[:, 1] > 0)\n",
    "y_xor = np.where(y_xor, 1, -1)\n",
    "plt.scatter(X_xor[y_xor==1, 0], X_xor[y_xor==1, 1],\n",
    "c='b', marker='x', label='1')\n",
    "plt.scatter(X_xor[y_xor==-1, 0], X_xor[y_xor==-1, 1],\n",
    "c='r', marker='s', label='-1')\n",
    "plt.ylim(-3.0)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(kernel='rbf', random_state=0, gamma=0.10, C=10.0)\n",
    "svm.fit(X_xor, y_xor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_decision_regions(X_xor, y_xor, classifier=svm)\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='svm_rbf.png', width = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression is used for predicting continuous responses, that is a value within a range, e.g. the height of students in a class is not fixed but rather a sliding scale of all possible human heights. In a legal context, ML regression algorithms could be used to predict the ideal fee quote for a matter with X, Y and Z variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='svm_reg.png', width = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike supervised learning, unsupervised learning does not require labelled data. This is because unsupervised learning techniques are designed to identify patterns inherent in the structure of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, in a legal context, you might use an unsupervised learning algorithm to identify logical groupings of contracts based on their shared syntax. Upon further human inspection, these groupings might reveal useful insights, e.g. documents with certain counterparties being more similar than not vs. documents without other counterparties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA (Principal Component Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
    "# load dataset into Pandas DataFrame\n",
    "df = pd.read_csv(url, names=['sepal length','sepal width','petal length','petal width','target'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "features = ['sepal length', 'sepal width', 'petal length', 'petal width']\n",
    "# Separating out the features\n",
    "x = df.loc[:, features].values\n",
    "# Separating out the target\n",
    "y = df.loc[:,['target']].values\n",
    "# Standardizing the features\n",
    "x = StandardScaler().fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(x)\n",
    "principalDf = pd.DataFrame(data = principalComponents, columns = ['principal component 1', 'principal component 2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalDf = pd.concat([principalDf, df[['target']]], axis = 1)\n",
    "finalDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "ax.set_title('2 component PCA', fontsize = 20)\n",
    "targets = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']\n",
    "colors = ['r', 'g', 'b']\n",
    "for target, color in zip(targets,colors):\n",
    "    indicesToKeep = finalDf['target'] == target\n",
    "    ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1'],\n",
    "               finalDf.loc[indicesToKeep, 'principal component 2'], \n",
    "               c = color,\n",
    "               s = 50)\n",
    "ax.legend(targets)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t-SNE (t-distributed stochastic neighbor embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/clustering.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://yann.lecun.com/exdb/mnist/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='mnist.png', width = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Means clustering on the handwritten digits data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "data, labels = load_digits(return_X_y=True)\n",
    "(n_samples, n_features), n_digits = data.shape, np.unique(labels).size\n",
    "\n",
    "print(\n",
    "    f\"# digits: {n_digits}; # samples: {n_samples}; # features {n_features}\"\n",
    ")\n",
    "\n",
    "# %%\n",
    "# Define our evaluation benchmark\n",
    "# -------------------------------\n",
    "#\n",
    "# We will first our evaluation benchmark. During this benchmark, we intend to\n",
    "# compare different initialization methods for KMeans. Our benchmark will:\n",
    "#\n",
    "# * create a pipeline which will scale the data using a\n",
    "#   :class:`~sklearn.preprocessing.StandardScaler`;\n",
    "# * train and time the pipeline fitting;\n",
    "# * measure the performance of the clustering obtained via different metrics.\n",
    "from time import time\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "cmap = cm.autumn\n",
    "norm = Normalize(vmin=0, vmax=9)\n",
    "colors = [cmap(norm(label)) for label in labels]\n",
    "\n",
    "def bench_k_means(kmeans, name, data, labels):\n",
    "    \"\"\"Benchmark to evaluate the KMeans initialization methods.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    kmeans : KMeans instance\n",
    "        A :class:`~sklearn.cluster.KMeans` instance with the initialization\n",
    "        already set.\n",
    "    name : str\n",
    "        Name given to the strategy. It will be used to show the results in a\n",
    "        table.\n",
    "    data : ndarray of shape (n_samples, n_features)\n",
    "        The data to cluster.\n",
    "    labels : ndarray of shape (n_samples,)\n",
    "        The labels used to compute the clustering metrics which requires some\n",
    "        supervision.\n",
    "    \"\"\"\n",
    "    t0 = time()\n",
    "    estimator = make_pipeline(StandardScaler(), kmeans).fit(data)\n",
    "    fit_time = time() - t0\n",
    "    results = [name, fit_time, estimator[-1].inertia_]\n",
    "\n",
    "    # Define the metrics which require only the true labels and estimator\n",
    "    # labels\n",
    "    clustering_metrics = [\n",
    "        metrics.homogeneity_score,\n",
    "        metrics.completeness_score,\n",
    "        metrics.v_measure_score,\n",
    "        metrics.adjusted_rand_score,\n",
    "        metrics.adjusted_mutual_info_score,\n",
    "    ]\n",
    "    results += [m(labels, estimator[-1].labels_) for m in clustering_metrics]\n",
    "\n",
    "    # The silhouette score requires the full dataset\n",
    "    results += [\n",
    "        metrics.silhouette_score(data, estimator[-1].labels_,\n",
    "                                 metric=\"euclidean\", sample_size=300,)\n",
    "    ]\n",
    "\n",
    "    # Show the results\n",
    "    formatter_result = (\"{:9s}\\t{:.3f}s\\t{:.0f}\\t{:.3f}\\t{:.3f}\"\n",
    "                        \"\\t{:.3f}\\t{:.3f}\\t{:.3f}\\t{:.3f}\")\n",
    "    print(formatter_result.format(*results))\n",
    "\n",
    "\n",
    "# %%\n",
    "# Run the benchmark\n",
    "# -----------------\n",
    "#\n",
    "# We will compare three approaches:\n",
    "#\n",
    "# * an initialization using `kmeans++`. This method is stochastic and we will\n",
    "#   run the initialization 4 times;\n",
    "# * a random initialization. This method is stochastic as well and we will run\n",
    "#   the initialization 4 times;\n",
    "# * an initialization based on a :class:`~sklearn.decomposition.PCA`\n",
    "#   projection. Indeed, we will use the components of the\n",
    "#   :class:`~sklearn.decomposition.PCA` to initialize KMeans. This method is\n",
    "#   deterministic and a single initialization suffice.\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "print(82 * '_')\n",
    "print('init\\t\\ttime\\tinertia\\thomo\\tcompl\\tv-meas\\tARI\\tAMI\\tsilhouette')\n",
    "\n",
    "kmeans = KMeans(init=\"k-means++\", n_clusters=n_digits, n_init=4,\n",
    "                random_state=0)\n",
    "bench_k_means(kmeans=kmeans, name=\"k-means++\", data=data, labels=labels)\n",
    "\n",
    "kmeans = KMeans(init=\"random\", n_clusters=n_digits, n_init=4, random_state=0)\n",
    "bench_k_means(kmeans=kmeans, name=\"random\", data=data, labels=labels)\n",
    "\n",
    "pca = PCA(n_components=n_digits).fit(data)\n",
    "kmeans = KMeans(init=pca.components_, n_clusters=n_digits, n_init=1)\n",
    "bench_k_means(kmeans=kmeans, name=\"PCA-based\", data=data, labels=labels)\n",
    "\n",
    "print(82 * '_')\n",
    "\n",
    "# %%\n",
    "# Visualize the results on PCA-reduced data\n",
    "# -----------------------------------------\n",
    "#\n",
    "# :class:`~sklearn.decomposition.PCA` allows to project the data from the\n",
    "# original 64-dimensional space into a lower dimensional space. Subsequently,\n",
    "# we can use :class:`~sklearn.decomposition.PCA` to project into a\n",
    "# 2-dimensional space and plot the data and the clusters in this new space.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "reduced_data = PCA(n_components=2).fit_transform(data)\n",
    "kmeans = KMeans(init=\"k-means++\", n_clusters=n_digits, n_init=4)\n",
    "kmeans.fit(reduced_data)\n",
    "\n",
    "# Step size of the mesh. Decrease to increase the quality of the VQ.\n",
    "h = .02     # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "x_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1\n",
    "y_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "# Obtain labels for each point in mesh. Use last trained model.\n",
    "Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(1)\n",
    "plt.clf()\n",
    "plt.imshow(Z, interpolation=\"nearest\",\n",
    "           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "           cmap=plt.cm.Paired, aspect=\"auto\", origin=\"lower\")\n",
    "\n",
    "plt.plot(reduced_data[:, 0], reduced_data[:, 1], 'k.', markersize=2)\n",
    "# Plot the centroids as a white X\n",
    "centroids = kmeans.cluster_centers_\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], marker=\"x\", s=169, linewidths=3,\n",
    "            color=\"w\", zorder=10)\n",
    "plt.title(\"K-means clustering on the digits dataset (PCA-reduced data)\\n\"\n",
    "          \"Centroids are marked with white cross\")\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP (multi-layer perceptron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='mlp.png', width = 800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(y_train[21])\n",
    "plt.imshow(X_train[21], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train[122])\n",
    "plt.imshow(X_train[122], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='gray_rgb.png', width = 900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"max: \", X_train.max())\n",
    "print(\"min: \", X_train.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32') / 255\n",
    "X_test = X_test.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"max: \", X_train.max())\n",
    "print(\"min: \", X_train.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='norm.jpg', width = 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 28 x 28 --> 784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train: \", X_train.shape)\n",
    "print(\"X_test: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], 784)\n",
    "X_test = X_test.reshape(X_test.shape[0], 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train: \", X_train.shape)\n",
    "print(\"X_test: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-Hot vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Image(filename='one_hot.jpg', width = 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"y_train: \", y_train.shape)\n",
    "print(\"y_test: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"y_train: \", y_train.shape)\n",
    "print(\"y_test: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train[122])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip = Input(shape=(784,), name=\"input\")\n",
    "x = Dense(units=512, name=\"hiddel_layer\", activation=\"relu\")(ip)\n",
    "op = Dense(units=10, name=\"prediction\", activation=\"softmax\")(x)\n",
    "\n",
    "model = Model(inputs=ip, outputs=op, name=\"full_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(784 * 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(784 * 512 + 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='sigmoid.jpg', width = 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='relu.png', width = 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='softmax.png', width = 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "optimizer = Adam(learning_rate, amsgrad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='optimizers.gif', width = 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train,\n",
    "                   y_train,\n",
    "                   batch_size=100,\n",
    "                   epochs=5,\n",
    "                   verbose=1,\n",
    "                   validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.gca().legend(('loss','val_loss'))\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.gca().legend(('acc','val_accuracy'))\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='mnist_heatmap.png', width = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin = X_test[121].reshape(28, 28)\n",
    "print(origin.shape)\n",
    "plt.imshow(origin, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(X_test[121].reshape(1, -1), batch_size=1)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(10), prediction[0])\n",
    "plt.xticks(range(10), range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin = X_test[21].reshape(28, 28)\n",
    "print(origin.shape)\n",
    "plt.imshow(origin, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(X_test[21].reshape(1, -1), batch_size=1)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(10), prediction[0])\n",
    "plt.xticks(range(10), range(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.gwern.net/Tanks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='overfitting.png', width = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Image(filename='model_comp.png', width = 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip = Input(shape=(784,), name=\"input\")\n",
    "x = Dense(units=2048, name=\"hiddel_layer\", activation=\"relu\")(ip)\n",
    "x = Dense(units=2048, name=\"hiddel_layer_2\", activation=\"relu\")(x)\n",
    "x = Dense(units=512, name=\"hiddel_layer_3\", activation=\"relu\")(x)\n",
    "op = Dense(units=10, name=\"prediction\", activation=\"softmax\")(x)\n",
    "\n",
    "model = Model(inputs=ip, outputs=op, name=\"full_model\")\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = Adam(learning_rate, amsgrad=True)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train[:1000],\n",
    "                   y_train[:1000],\n",
    "                   batch_size=100,\n",
    "                   epochs=10,\n",
    "                   verbose=1,\n",
    "                   validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.gca().legend(('acc','val_accuracy'))\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization: L1, L2, augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Image(filename='augmentation.png', width = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=5&networkShape=3,2&seed=0.25772&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning ( “DL“) is a subtype of machine learning. DL can process a wider range of data resources, requires less data preprocessing by humans (e.g. feature labelling), and can sometimes produce more accurate results than traditional ML approaches (although it requires a larger amount of data to do so)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Image(filename='MLvsDL.png', width = 900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='sheep.jpeg', width = 900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='3d_detection.jpg', width = 900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='pose.png', width = 900)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN (convolutional neural network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='convolution.gif', width = 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='pooling.gif', width = 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='cnn.jpeg', width = 900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='cnn_2.png', width = 900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Normalize the images.\n",
    "train_images = (train_images / 255) - 0.5\n",
    "test_images = (test_images / 255) - 0.5\n",
    "\n",
    "# Reshape the images.\n",
    "train_images = np.expand_dims(train_images, axis=3)\n",
    "test_images = np.expand_dims(test_images, axis=3)\n",
    "\n",
    "num_kernels = 8  # filters\n",
    "kernel_size = 3\n",
    "pool_size = 2\n",
    "\n",
    "# Build the model.\n",
    "model = Sequential([Conv2D(num_kernels, kernel_size, input_shape=(28, 28, 1)),\n",
    "                    MaxPooling2D(pool_size=pool_size),\n",
    "                    Flatten(),\n",
    "                    Dense(10, activation='softmax'),\n",
    "                    ])\n",
    "\n",
    "# Compile the model.\n",
    "model.compile('adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model.\n",
    "model.fit(train_images,\n",
    "          to_categorical(train_labels),\n",
    "          epochs=5,\n",
    "          validation_data=(test_images, to_categorical(test_labels)),\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN (Recurrent neural network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='rnn.gif', width = 900)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM (Long short-term memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='lstm.gif', width = 900)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP (Natural Language Processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='nlp.jpg', width = 700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
